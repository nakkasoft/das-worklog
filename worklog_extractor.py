import os
import sys
import csv
import time
import datetime as dt
import requests
from requests.auth import HTTPBasicAuth
import json

# UTF-8 ì¸ì½”ë”© ì„¤ì •
import codecs
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# ì‹œìŠ¤í…œë³„ ê¸°ë³¸ URL ì„¤ì •
JIRA_BASE = "http://jira.lge.com/issue"
CONFLUENCE_BASE = "http://collab.lge.com/main"
GERRIT_URLS = {
    "NA": "http://vgit.lge.com/na",
    "EU": "http://vgit.lge.com/eu", 
    "AS": "http://vgit.lge.com/as"
}

# ì‹œê°„ ì„¤ì •
NOW_UTC = dt.datetime.now(dt.UTC).replace(tzinfo=None)
SINCE = NOW_UTC - dt.timedelta(days=1)

def iso_to_dt(s):
    """ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    try:
        if s.endswith("Z"):
            return dt.datetime.fromisoformat(s.replace("Z","+00:00")).replace(tzinfo=None)
        if "+" in s and s[-5] in ["+", "-"] and s[-3] != ":":
            s = s[:-5] + s[-5:-2] + ":" + s[-2:]
        # Gerrit ì‹œê°„ í˜•ì‹: "2025-09-18 02:47:20.000000000"
        if "." in s and len(s.split(".")[-1]) > 6:
            # ë‚˜ë…¸ì´ˆë¥¼ ë§ˆì´í¬ë¡œì´ˆë¡œ ë³€í™˜
            parts = s.split(".")
            microseconds = parts[1][:6]
            s = f"{parts[0]}.{microseconds}"
        return dt.datetime.fromisoformat(s).replace(tzinfo=None)
    except Exception as e:
        # Gerrit ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
        try:
            return dt.datetime.strptime(s.split(".")[0], "%Y-%m-%d %H:%M:%S")
        except:
            return None

def extract_text_from_adf(adf_content):
    """
    Atlassian Document Format(ADF)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
    
    Args:
        adf_content (dict): ADF í˜•ì‹ì˜ ì½˜í…ì¸ 
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    if not adf_content or not isinstance(adf_content, dict):
        return ""
    
    def extract_text_recursive(node):
        """ADF ë…¸ë“œì—ì„œ ì¬ê·€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_parts = []
        
        if isinstance(node, dict):
            # í…ìŠ¤íŠ¸ ë…¸ë“œì¸ ê²½ìš°
            if node.get("type") == "text":
                text_parts.append(node.get("text", ""))
            
            # ë‹¤ë¥¸ ë…¸ë“œ íƒ€ì…ë“¤ ì²˜ë¦¬
            elif node.get("type") in ["paragraph", "heading", "bulletList", "orderedList", "listItem"]:
                if "content" in node:
                    for child in node["content"]:
                        text_parts.append(extract_text_recursive(child))
            
            # hardBreakëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ ì²˜ë¦¬
            elif node.get("type") == "hardBreak":
                text_parts.append("\n")
                
        elif isinstance(node, list):
            for item in node:
                text_parts.append(extract_text_recursive(item))
        
        return " ".join(text_parts)
    
    try:
        # ADFì˜ ìµœìƒìœ„ contentì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if "content" in adf_content:
            extracted = extract_text_recursive(adf_content["content"])
            return extracted.strip()
        else:
            return str(adf_content).strip()
    except Exception as e:
        # ADF íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜
        return str(adf_content)[:500]  # ê¸¸ì´ ì œí•œ

# =============================================================================
# JIRA ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =============================================================================

def collect_jira_data(username, token):
    """
    Jira ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        username (str): Jira ì‚¬ìš©ìëª…
        token (str): Jira API í† í°
        
    Returns:
        list: Jira í™œë™ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {token}"
    }
    
    try:
        # ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸°
        r = requests.get(f"{JIRA_BASE}/rest/api/2/myself", headers=headers)
        r.raise_for_status()
        user_data = r.json()
        
        # ìµœê·¼ 1ì¼ê°„ì˜ í™œë™ ê²€ìƒ‰
        jql = "(updated >= -1d) AND (assignee = currentUser() OR reporter = currentUser() OR comment ~ currentUser() OR worklogAuthor = currentUser())"
        #jql = "Key = CLUSTWORK-16128"
        
        params = {
            "jql": jql,
            "fields": "key,summary,updated,status,assignee,reporter,created,description",
            "maxResults": 500
        }
        
        r = requests.get(f"{JIRA_BASE}/rest/api/2/search", headers=headers, params=params)
        r.raise_for_status()
        
        # JSON ì‘ë‹µ ê²€ì¦
        try:
            data = r.json()
        except json.JSONDecodeError:
            print(f"âŒ Jira API ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {r.text[:200]}")
            return []
        
        # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
        if not isinstance(data, dict):
            print(f"âŒ Jira API ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(data)}")
            return []
        
        # issues í•„ë“œ ê²€ì¦
        if "issues" not in data:
            print(f"âŒ Jira API ì‘ë‹µì— 'issues' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(data.keys())}")
            return []
        
        issues = data.get("issues", [])
        
        # issuesê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if not isinstance(issues, list):
            print(f"âŒ 'issues' í•„ë“œê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(issues)}")
            return []
        
        print(f"âœ… Jiraì—ì„œ {len(issues)}ê°œì˜ ì´ìŠˆë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        
        activities = []
        for issue in issues:
            try:
                # ê¸°ë³¸ í•„ë“œë“¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                fields = issue.get("fields", {})
                if not fields:
                    continue
                
                # ì‹œê°„ í•„ë“œ ì²˜ë¦¬
                updated_str = fields.get("updated")
                created_str = fields.get("created")
                
                if not updated_str:
                    continue
                    
                updated_dt = iso_to_dt(updated_str)
                created_dt = iso_to_dt(created_str) if created_str else None
                
                if updated_dt and updated_dt >= SINCE:
                    # description í•„ë“œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    description = ""
                    description_field = fields.get("description")
                    if description_field:
                        if isinstance(description_field, str):
                            description = description_field
                        elif isinstance(description_field, dict):
                            # ADF(Atlassian Document Format) í˜•ì‹ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            description = extract_text_from_adf(description_field)
                    
                    # assignee ì•ˆì „ ì²˜ë¦¬
                    assignee_info = fields.get("assignee")
                    if assignee_info is None:
                        assignee_name = "Unassigned"
                    else:
                        assignee_name = assignee_info.get("displayName", "Unknown") if isinstance(assignee_info, dict) else "Unknown"
                    
                    # reporter ì•ˆì „ ì²˜ë¦¬
                    reporter_info = fields.get("reporter")
                    if reporter_info is None:
                        reporter_name = "Unknown"
                    else:
                        reporter_name = reporter_info.get("displayName", "Unknown") if isinstance(reporter_info, dict) else "Unknown"
                    
                    # status í•„ë“œ ì•ˆì „ ì²˜ë¦¬
                    status_info = fields.get("status")
                    if status_info is None:
                        status_name = "Unknown"
                    else:
                        status_name = status_info.get("name", "Unknown") if isinstance(status_info, dict) else "Unknown"
                    
                    # summary í•„ë“œ ì•ˆì „ ì²˜ë¦¬
                    summary = fields.get("summary", "No Summary")
                    
                    # issue key ì•ˆì „ ì²˜ë¦¬
                    issue_key = issue.get("key", "Unknown")
                    
                    activities.append({
                        "source": "jira",
                        "type": "issue_activity",
                        "issue_key": issue_key,
                        "summary": summary,
                        "description": description,
                        "status": status_name,
                        "assignee": assignee_name,
                        "reporter": reporter_name,
                        "created": created_str or "Unknown",
                        "updated": updated_str,
                        "url": f"{JIRA_BASE}/browse/{issue_key}"
                    })
                    
            except Exception as e:
                print(f"âš ï¸ ì´ìŠˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (í‚¤: {issue.get('key', 'Unknown')}): {e}")
                continue
        
        return activities
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Jira ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ Jira JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
        return []
    except KeyError as e:
        print(f"âŒ Jira ì‘ë‹µì—ì„œ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {e}")
        return []
    except Exception as e:
        print(f"âŒ Jira ë°ì´í„° ìˆ˜ì§‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []

# =============================================================================
# CONFLUENCE ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =============================================================================

def collect_confluence_data(username, token):
    """
    Confluence ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        username (str): Confluence ì‚¬ìš©ìëª…
        token (str): Confluence API í† í°
        
    Returns:
        list: Confluence í™œë™ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {token}"
    }
    
    try:
        since_str = SINCE.strftime("%Y-%m-%d")
        params = {
            "cql": f"contributor = currentUser() AND lastModified >= '{since_str}'",
            "limit": 500
        }
        
        r = requests.get(f"{CONFLUENCE_BASE}/rest/api/content/search", headers=headers, params=params)
        r.raise_for_status()
        data = r.json()
        
        activities = []
        for page in data.get("results", []):
            activities.append({
                "source": "confluence",
                "type": "page_activity", 
                "page_id": page["id"],
                "title": page["title"],
                "space": page.get("space", {}).get("name", ""),
                "space_key": page.get("space", {}).get("key", ""),
                "last_modified": page.get("version", {}).get("when", ""),
                "url": f"{CONFLUENCE_BASE}/pages/viewpage.action?pageId={page['id']}"
            })
        
        return activities
        
    except Exception as e:
        print(f"âŒ Confluence ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

# =============================================================================
# GERRIT ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =============================================================================

def collect_gerrit_data(username, tokens):
    """
    Gerrit ë°ì´í„° ìˆ˜ì§‘ (ëª¨ë“  ì„œë²„)
    
    Args:
        username (str): Gerrit ì‚¬ìš©ìëª…
        tokens (dict): ì„œë²„ë³„ Gerrit í† í° ë”•ì…”ë„ˆë¦¬ {"NA": "token1", "EU": "token2", "AS": "token3"}
        
    Returns:
        tuple: (reviews, comments) - ë¦¬ë·° ë°ì´í„°ì™€ ëŒ“ê¸€ ë°ì´í„°
    """
    all_reviews = []
    all_comments = []
    
    for server, token in tokens.items():
        if server not in GERRIT_URLS:
            continue
            
        try:
            reviews, comments = collect_gerrit_server_data(username, token, server)
            all_reviews.extend(reviews)
            all_comments.extend(comments)
        except Exception as e:
            print(f"âŒ Gerrit {server} ì„œë²„ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    return all_reviews, all_comments

def collect_gerrit_server_data(username, token, server="NA"):
    """
    íŠ¹ì • Gerrit ì„œë²„ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        username (str): Gerrit ì‚¬ìš©ìëª…
        token (str): Gerrit API í† í°
        server (str): ì„œë²„ ì´ë¦„ (NA, EU, AS)
        
    Returns:
        tuple: (reviews, comments) - í•´ë‹¹ ì„œë²„ì˜ ë¦¬ë·° ë°ì´í„°ì™€ ëŒ“ê¸€ ë°ì´í„°
    """
    auth = HTTPBasicAuth(username, token)
    base_url = GERRIT_URLS[server]
    
    all_reviews = []
    all_comments = []
    
    # ìµœê·¼ 1ì¼ê°„ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
    since_str = SINCE.strftime("%Y-%m-%d")
    queries = [
        f"owner:{username} after:{since_str}",  # ë‚´ê°€ ì‘ì„±í•œ ë¦¬ë·°
        f"reviewer:{username} after:{since_str}",  # ë‚´ê°€ ë¦¬ë·°í•œ ê²ƒë“¤
        f"commentby:{username} after:{since_str}"  # ë‚´ê°€ ëŒ“ê¸€ ë‹¨ ê²ƒë“¤
    ]
    
    processed_changes = set()  # ì¤‘ë³µ ë°©ì§€
    
    for query in queries:
        try:
            changes = search_gerrit_changes(auth, base_url, query, limit=500)
            
            for change in changes:
                change_id = change.get("id", "")
                if change_id in processed_changes:
                    continue
                processed_changes.add(change_id)
                
                change_number = change.get("_number", "")
                subject = change.get("subject", "")
                status = change.get("status", "")
                owner = change.get("owner", {})
                created = change.get("created", "")
                updated = change.get("updated", "")
                project = change.get("project", "")
                branch = change.get("branch", "")
                
                # ì‹œê°„ í•„í„°ë§
                updated_dt = iso_to_dt(updated)
                if not updated_dt or updated_dt < SINCE:
                    continue
                
                # ë‚´ê°€ ì†Œìœ ìì¸ ê²½ìš° (ë‚´ê°€ ì‘ì„±í•œ ë¦¬ë·°)
                owner_username = owner.get("username", owner.get("name", ""))
                if owner_username == username:
                    all_reviews.append({
                        "source": f"gerrit_{server.lower()}",
                        "type": "review_created",
                        "change_id": change_id,
                        "change_number": change_number,
                        "subject": subject,
                        "status": status,
                        "project": project,
                        "branch": branch,
                        "created": created,
                        "updated": updated,
                        "url": f"{base_url}/c/{change_number}"
                    })
                
                # ë©”ì‹œì§€ í™•ì¸ (ë‚´ ëŒ“ê¸€)
                messages = change.get("messages", [])
                for message in messages:
                    author = message.get("author", {})
                    author_username = author.get("username", author.get("name", ""))
                    message_date = message.get("date", "")
                    
                    if author_username == username:
                        message_dt = iso_to_dt(message_date)
                        if message_dt and message_dt >= SINCE:
                            all_comments.append({
                                "source": f"gerrit_{server.lower()}",
                                "type": "review_comment",
                                "change_id": change_id,
                                "change_number": change_number,
                                "subject": subject,
                                "project": project,
                                "message": message.get("message", ""),
                                "created": message_date,
                                "url": f"{base_url}/c/{change_number}"
                            })
                
                # ìƒì„¸ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
                try:
                    detailed_comments = get_gerrit_comments(auth, base_url, change_id)
                    
                    for file_path, comments_list in detailed_comments.items():
                        for comment in comments_list:
                            author = comment.get("author", {})
                            author_username = author.get("username", author.get("name", ""))
                            comment_updated = comment.get("updated", "")
                            
                            if author_username == username:
                                comment_dt = iso_to_dt(comment_updated)
                                if comment_dt and comment_dt >= SINCE:
                                    all_comments.append({
                                        "source": f"gerrit_{server.lower()}",
                                        "type": "code_comment",
                                        "change_id": change_id,
                                        "change_number": change_number,
                                        "subject": subject,
                                        "project": project,
                                        "file_path": file_path,
                                        "line": comment.get("line", ""),
                                        "message": comment.get("message", ""),
                                        "created": comment_updated,
                                        "url": f"{base_url}/c/{change_number}"
                                    })
                except Exception as e:
                    print(f"    âš ï¸ {change_id} ìƒì„¸ ëŒ“ê¸€ ì˜¤ë¥˜: {e}")
                
                time.sleep(0.1)  # API ë¶€í•˜ ë°©ì§€
                
        except Exception as e:
            print(f"  âš ï¸ ì¿¼ë¦¬ '{query}' ì˜¤ë¥˜: {e}")
    
    return all_reviews, all_comments

def gerrit_request(url, auth, params=None):
    """Gerrit API ìš”ì²­"""
    try:
        response = requests.get(url, auth=auth, params=params, timeout=30)
        
        # Gerritì€ ë³´ì•ˆìƒ ")]}'" ì ‘ë‘ì‚¬ë¥¼ ì‘ë‹µì— ì¶”ê°€í•¨
        text = response.text
        if text.startswith(")]}'\n"):
            text = text[5:]
        
        response.raise_for_status()
        
        # UTF-8 ì¸ì½”ë”© í™•ì¸
        if response.encoding:
            response.encoding = 'utf-8'
        
        return json.loads(text) if text.strip() else []
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Gerrit ìš”ì²­ ì˜¤ë¥˜: {e}")
        return []
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []

def search_gerrit_changes(auth, base_url, query, limit=1000):
    """Gerritì—ì„œ ë³€ê²½ì‚¬í•­ ê²€ìƒ‰"""
    url = f"{base_url}/a/changes/"
    params = {
        "q": query,
        "o": ["DETAILED_ACCOUNTS", "DETAILED_LABELS", "MESSAGES", "CURRENT_REVISION"],
        "n": limit
    }
    
    print(f"  Gerrit ê²€ìƒ‰: {query}")
    return gerrit_request(url, auth, params)

def get_gerrit_comments(auth, base_url, change_id):
    """íŠ¹ì • ë³€ê²½ì‚¬í•­ì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{base_url}/a/changes/{change_id}/comments"
    return gerrit_request(url, auth)

# =============================================================================
# ë°ì´í„° ê°€ê³µ ë° ë¶„ì„ í•¨ìˆ˜
# =============================================================================

def process_activity_data(jira_data, confluence_data, gerrit_reviews, gerrit_comments):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ í†µí•© í™œë™ íƒ€ì„ë¼ì¸ ìƒì„±
    
    Args:
        jira_data (list): Jira í™œë™ ë°ì´í„°
        confluence_data (list): Confluence í™œë™ ë°ì´í„°
        gerrit_reviews (list): Gerrit ë¦¬ë·° ë°ì´í„°
        gerrit_comments (list): Gerrit ëŒ“ê¸€ ë°ì´í„°
        
    Returns:
        list: í†µí•©ëœ í™œë™ íƒ€ì„ë¼ì¸ ë°ì´í„°
    """
    all_activities = []
    
    # Jira í™œë™ ë³€í™˜
    for activity in jira_data:
        # descriptionì´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
        description_preview = activity.get("description", "")
        if len(description_preview) > 100:
            description_preview = description_preview[:100] + "..."
        
        all_activities.append({
            "timestamp": activity["updated"],
            "source": "JIRA",
            "type": "ì´ìŠˆ í™œë™",
            "reference": activity["issue_key"],
            "description": f"[{activity['status']}] {activity['summary']}",
            "content": description_preview if description_preview else activity["url"],
            "full_description": activity.get("description", ""),  # ì „ì²´ ì„¤ëª… ë³´ê´€
            "raw_data": activity
        })
    
    # Confluence í™œë™ ë³€í™˜
    for activity in confluence_data:
        all_activities.append({
            "timestamp": activity.get("last_modified", dt.datetime.now().isoformat()),
            "source": "CONFLUENCE",
            "type": "í˜ì´ì§€ í™œë™",
            "reference": activity["page_id"],
            "description": f"{activity['space']}: {activity['title']}",
            "content": activity["url"],
            "raw_data": activity
        })
    
    # Gerrit ë¦¬ë·° ë³€í™˜
    for review in gerrit_reviews:
        all_activities.append({
            "timestamp": review["updated"],
            "source": review["source"].upper(),
            "type": "ì½”ë“œë¦¬ë·° ìƒì„±",
            "reference": f"{review['project']}#{review['change_number']}",
            "description": f"[{review['status']}] {review['subject']}",
            "content": review["url"],
            "raw_data": review
        })
    
    # Gerrit ëŒ“ê¸€ ë³€í™˜
    for comment in gerrit_comments:
        comment_type = "ì½”ë“œ ëŒ“ê¸€" if comment["type"] == "code_comment" else "ë¦¬ë·° ëŒ“ê¸€"
        all_activities.append({
            "timestamp": comment["created"],
            "source": comment["source"].upper(),
            "type": comment_type,
            "reference": f"{comment['project']}#{comment['change_number']}",
            "description": f"{comment['subject']}",
            "content": comment["message"][:100] + "..." if len(comment["message"]) > 100 else comment["message"],
            "raw_data": comment
        })
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_activities.sort(key=lambda x: x["timestamp"], reverse=True)
    
    return all_activities

def generate_activity_summary(activities):
    """
    í™œë™ ë°ì´í„° ìš”ì•½ ìƒì„±
    
    Args:
        activities (list): í™œë™ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        dict: í™œë™ ìš”ì•½ ì •ë³´
    """
    summary = {
        "total_activities": len(activities),
        "by_source": {},
        "by_type": {},
        "date_range": {
            "start": None,
            "end": None
        }
    }
    
    dates = []
    for activity in activities:
        # ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸
        source = activity["source"]
        summary["by_source"][source] = summary["by_source"].get(source, 0) + 1
        
        # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        activity_type = activity["type"]
        summary["by_type"][activity_type] = summary["by_type"].get(activity_type, 0) + 1
        
        # ë‚ ì§œ ìˆ˜ì§‘
        try:
            date = iso_to_dt(activity["timestamp"])
            if date:
                dates.append(date)
        except:
            pass
    
    # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    if dates:
        dates.sort()
        summary["date_range"]["start"] = dates[0].strftime("%Y-%m-%d")
        summary["date_range"]["end"] = dates[-1].strftime("%Y-%m-%d")
    
    return summary

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def write_csv(path, headers, rows):
    """CSV íŒŒì¼ ì‘ì„± (UTF-8 BOM í¬í•¨)"""
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        for r in rows:
            writer.writerow({h: r.get(h, "") for h in headers})
    print(f"âœ“ {path} ì‘ì„± ì™„ë£Œ ({len(rows)}ê°œ í–‰)")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì˜ˆì œ)
# =============================================================================

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì‹¤ì œ í† í°ê³¼ ì‚¬ìš©ìëª…ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©
    """
    print("=== Jira & Confluence & Gerrit í†µí•© í™œë™ ì¶”ì¶œê¸° ===")
    print(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ 1ì¼ ({SINCE.strftime('%Y-%m-%d')} ì´í›„)")
    
    # ì‹¤ì œ ì‚¬ìš©ì ì •ë³´ ì„¤ì • (ì—¬ê¸°ì„œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©)
    USERNAME = ""
    JIRA_TOKEN = ""
    CONFLUENCE_TOKEN = ""
    GERRIT_TOKENS = {
        "NA": "",
        "EU": "",
        "AS": ""
    }
    
    # 1. ê° ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    print("\n=== ë°ì´í„° ìˆ˜ì§‘ ===")
    
    print("JIRA ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    jira_data = collect_jira_data(USERNAME, JIRA_TOKEN)
    print(f"âœ“ Jira í™œë™: {len(jira_data)}ê°œ")
    
    print("Confluence ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    confluence_data = collect_confluence_data(USERNAME, CONFLUENCE_TOKEN)
    print(f"âœ“ Confluence í™œë™: {len(confluence_data)}ê°œ")
    
    print("Gerrit ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    gerrit_reviews, gerrit_comments = collect_gerrit_data(USERNAME, GERRIT_TOKENS)
    print(f"âœ“ Gerrit ë¦¬ë·°: {len(gerrit_reviews)}ê°œ")
    print(f"âœ“ Gerrit ëŒ“ê¸€: {len(gerrit_comments)}ê°œ")
    
    # 2. ë°ì´í„° ê°€ê³µ
    print("\n=== ë°ì´í„° ê°€ê³µ ===")
    integrated_activities = process_activity_data(jira_data, confluence_data, gerrit_reviews, gerrit_comments)
    activity_summary = generate_activity_summary(integrated_activities)
    
    print(f"í†µí•© í™œë™: {activity_summary['total_activities']}ê°œ")
    print("ì†ŒìŠ¤ë³„ í™œë™:")
    for source, count in activity_summary['by_source'].items():
        print(f"  - {source}: {count}ê°œ")
    
    # 3. CSV íŒŒì¼ ì¶œë ¥
    print("\n=== íŒŒì¼ ì¶œë ¥ ===")
    
    # Gerrit ë¦¬ë·° CSV
    if gerrit_reviews:
        write_csv("gerrit_reviews.csv",
                  ["source", "type", "change_id", "change_number", "subject", "status", "project", "branch", "created", "updated", "url"],
                  gerrit_reviews)
    
    # Gerrit ëŒ“ê¸€ CSV
    if gerrit_comments:
        write_csv("gerrit_comments.csv",
                  ["source", "type", "change_id", "change_number", "subject", "project", "file_path", "line", "message", "created", "url"],
                  gerrit_comments)
    
    # í†µí•© í™œë™ íƒ€ì„ë¼ì¸ CSV
    write_csv("integrated_activity_timeline.csv",
              ["timestamp", "source", "type", "reference", "description", "content", "full_description"],
              integrated_activities)
    
    print(f"\nğŸ‰ ì™„ë£Œ! ëª¨ë“  ë°ì´í„°ê°€ ìˆ˜ì§‘ ë° ê°€ê³µë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - í†µí•© í™œë™: {len(integrated_activities)}ê°œ")
    print(f"   - ê¸°ê°„: {activity_summary['date_range']['start']} ~ {activity_summary['date_range']['end']}")
    
    return {
        "jira_data": jira_data,
        "confluence_data": confluence_data,
        "gerrit_reviews": gerrit_reviews,
        "gerrit_comments": gerrit_comments,
        "integrated_activities": integrated_activities,
        "summary": activity_summary
    }

# ê°œë³„ í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì œ
def example_usage():
    """
    ê°œë³„ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ
    """
    # ì‚¬ìš©ì ì •ë³´
    username = "sangyeob.na"
    jira_token = "your_jira_token"
    confluence_token = "your_confluence_token"
    gerrit_tokens = {
        "NA": "your_na_token",
        "EU": "your_eu_token", 
        "AS": "your_as_token"
    }
    
    # 1. ê°œë³„ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    jira_activities = collect_jira_data(username, jira_token)
    confluence_activities = collect_confluence_data(username, confluence_token)
    gerrit_reviews, gerrit_comments = collect_gerrit_data(username, gerrit_tokens)
    
    # 2. ë°ì´í„° ê°€ê³µ
    integrated_data = process_activity_data(
        jira_activities, 
        confluence_activities, 
        gerrit_reviews, 
        gerrit_comments
    )
    
    # 3. ìš”ì•½ ì •ë³´ ìƒì„±
    summary = generate_activity_summary(integrated_data)
    
    return {
        "raw_data": {
            "jira": jira_activities,
            "confluence": confluence_activities,
            "gerrit_reviews": gerrit_reviews,
            "gerrit_comments": gerrit_comments
        },
        "processed_data": integrated_data,
        "summary": summary
    }

if __name__ == "__main__":
    main()
